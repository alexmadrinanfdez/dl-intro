{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQv2gotnAZur"
   },
   "source": [
    "<font size = 7> Deep Learning </font>\n",
    "\n",
    "*Alejandro Madriñán Fernández*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otp6Z8q-AZuz"
   },
   "source": [
    "![](https://www.etsit.upm.es/fileadmin/documentos/laescuela/la_escuela/galerias_fotograficas/Servicios/generales/logos/LOGO_ESCUELA/LOGO_ESCUELA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaeyE55mAZu0"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "*Deep Learning* is framed inside the *Machine Learning* environment.\n",
    "When traditional machine learning techniques lack strength, deep learning comes as an excellent alternative. The great flexibility their models achieve makes them great for complicated tasks.\n",
    "\n",
    "Models in the machine learning field are built to mimic a certain relationship between a **response** and a set of **predictors**. Given that the true relationship between the two parts, function $f(·)$, exists, the model will develop a function $\\hat{f}(·)$ that approximates $f(·)$ according to a certain internal method, dependant on the model.\n",
    "\n",
    "While traditional machine learning approaches involve making certain assumptions on the model and working over a set of pre-adapted features, the deep learning approach is to learn the form of $\\hat{f}(·)$ without making any specific assumptions or being restricted by a general shape. In the process, deep learning models are able to also perform the task known as *feature extraction*, this is, transforming the original predictors to fill the needs of the problem. Their downside is that the computations needed will become more expensive as the complexity increases, but that is only natural when facing a complex challenge.\n",
    "\n",
    "This report aims at presenting the structure of some of the more basic architectures inside deep learning, which are the basis of some more complicated state-of-the-art models.\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "One of the big claims of deep learning is that its models are able to reach a level of cognition comparable to that of the human mind (for a given task) because, in a sense, they tries to mimic the brain's behaviour. In fact, deep learning's models are often referred to as *neural networks*. As the name suggests, the basic learning unit in a deep learning model is a neuron.\n",
    "\n",
    "A deep learning neuron, much like a biological neuron, receives impulses (in this case they are not electrical) that can then activate said neuron. Unlike a biological neuron, deep learning neurons act first on the coming signal and only afterwards decide if to pass it on or deactivate.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mohammad_Bataineh5/publication/335190001/figure/fig1/AS:792206994587652@1565888274951/Schemes-for-human-brain-neuron-and-an-artificial-neural-network-ANN.png\" width=\"500\"/>\n",
    "\n",
    "Specifically, neurons apply a linear combination of their input singals $x_i$ to an activation function. It can be expressed as\n",
    "\n",
    "$$ y = h \\left( \\sum_i{(x_i w_i) + b} \\right) = h ( \\mathbf{x}^T \\mathbf{w} + b ) $$\n",
    "\n",
    "where $h(·)$ is the activation function that has as its input the product of the vector of inputs, $\\mathbf{x}$, and the vector of *weights* given to those inputs $\\mathbf{w}$. Additionally, before the activation function, a certain amount of bias ($b$) is added to the signal.\n",
    "\n",
    "There are a lot of different activation functions with different behaviours. Nevertheless, most of them are variations of the next three:\n",
    "\n",
    "- *Sigmoid* function. Traditional activation function used widely for its atractive computational properties. Its output $y$ verifies that $y \\in (0, 1)$.\n",
    "\n",
    "$$ h_{sigmoid}(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}} = \\sigma(x) $$\n",
    "\n",
    "- *Rectified Linear Unit* function. Commonly known as ReLU, its major advantage is that is able to deactivate neurons when the input is not significant.\n",
    "\n",
    "$$ h_{relu}(x) = \\bigg\\{ \\begin{matrix} x & x \\gt 0 \\\\ 0 & x \\leq 0 \\end{matrix} = \\max(0, x) $$\n",
    "\n",
    "- *Hyperbolic Tangent* function. Is similar to the sigmoid function but its output ranges from $y \\in (-1, 1)$, giving more freedom to the learning process.\n",
    "\n",
    "$$ h_{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{\\sinh(x)}{\\cosh(x)} = \\tanh(x) $$\n",
    "\n",
    "<!-- matplotlib plot -->\n",
    "<!-- <img src=\"\" alt=\"\" align=\"left\" width=\"\" style=\"padding: 0 15px; float: left;\"/> -->\n",
    "\n",
    "Neurons are connected with each other forming a network and the impulses change as they travel through the network.\n",
    "\n",
    "\n",
    "## Layered architecture\n",
    "\n",
    "Neural networks are constructed as a set of layers one on top of another. Every layer containing a defined number of neurons.\n",
    "The first and last layers are called input and output layer, and every other layer in between is a hidden layer.\n",
    "\n",
    "The name **deep** learning is inspired by the high number of hidden layers that in some cases these models have. As the depth of the model is measured with the number of layers.\n",
    "\n",
    "### Softmax layer\n",
    "\n",
    "As with the rest of machine learning problems, they can be divided in classification and regression, depending on the type of the response. But it is a common practice to turn regression problems into classification problems restricting a continuous response into N possible values.\n",
    "For classification settings, the output layer becomes a *softmax* layer. This layer's function is to express its outputs $y_j$ as probabilities. To do that, it computes\n",
    "\n",
    "$$ y_i = \\frac{e^{x_i}}{\\sum_{j = 1}^n e^{x_j}} $$\n",
    "\n",
    "where $n$ is the total number of inputs and each input $x_i$ is converted to its probability $y_i = p(x_i)$.\n",
    "These outputs fulfill the next two properties:\n",
    "\n",
    "1. The sum of all probabilities is the unit. $\\sum_{i = 1}^n y_i = 1$\n",
    "\n",
    "1. Probabilties are positive. $y_i \\in [0, 1] \\ \\forall i$\n",
    "\n",
    "## Optimization\n",
    "\n",
    "Deep learning problems can be formulated as *optimization* problems. The optimization strategies are based on minimizing a loss function or maximizing a gain function. Depending on the problem we have:\n",
    "\n",
    "- Classification: *maximum likelihood estimation* (MLE) or *cross-entropy*. The cross-entropy statistic can be computed from the MLE like this\n",
    "\n",
    "$$ CE = - \\log( \\theta_{MLE} ) $$\n",
    "\n",
    "- Regression: MSE (mean standard error) or MAE (mean average error). The average error based on either the $\\ell_2$ or $\\ell_1$ norm respectively.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "There is a large number of alternatives for learning the optimal set of weights for each neuron. And, as a general rule, there is no algorithm that outperforms the rest. In this regard, depending on your problem, there might even not be a better solution and you can achieve similar results with different algorithms.\n",
    "\n",
    "These methods, as a general rule, have in common that are based on moving in the opposite direction of the gradient of the cost function ($-\\frac{dy}{dx}$). There are several aspects to be taken into account when moving towards the minimum. In fact, most of the times is not possible to ensure that the algorithm will reach the **global** minimum but a **local** one (unless the cost function is *convex*). Here, we will just mention\n",
    "\n",
    "- the size of the step $\\alpha$. It's the distance traveled by the algorithm each iteration. An overly small value will make the algorithm slow while a big value may cause it to diverge. A usual compromise is to make its size adaptative, shrinking with each iteration $i$.\n",
    "\n",
    "$$ \\alpha_i = \\frac{\\alpha}{n \\sqrt{i}} $$\n",
    "\n",
    "- *momentum*. Tries to imitate the efect of actual physical momentum.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Optimizations algorithms, as a general rule, need to known the gradient of the loss (or gain) function. *Backpropagation* offers an efficient method to do it. It is based on the chain rule to compute partial derivatives.\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial g}{\\partial a} = \n",
    "\\frac{\\partial g}{\\partial z} \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x} \\cdots \n",
    "\\frac{\\partial b}{\\partial a} \n",
    "$$\n",
    "\n",
    "First, the algorithm sweeps through the network, starting from the inputs of the network until it reaches the end. Then it computes the goal function $g(·)$, and makes the way back using the chain rule to compute the gradients at each step of the way.\n",
    "\n",
    "The main idea is that the first derivatives can be reused in the different paths you can take to the beginnning.\n",
    "\n",
    "## Size of the data\n",
    "\n",
    "In deep learning the size of the data matters much more than in any other field of machine learning. This is to be expected since the number of hyperparameters the model needs to fine tune grows exponentially but, in most cases, the available data is not enough to reach the optimal behaviour of the model. With more data we could increases the performance.\n",
    "\n",
    "A roundabout way of improving the performance without increases the amount of data is *transfer learning*. It consists of using already trained networks for similar or more generic tasks and tune just the upper layers (the ones close to the output) with our problem-specific data. This can be reasoned as leaving the feature extraction stage as it is, because it has already been trained with plentiful data, and just tweak the final prediction stage.\n",
    "\n",
    "An alternative way of gathering more data is generating it from the original samples. This is called *data augmentation*. It involves preprocessing the original data to add noise or perform transformations that generate new samples. It has the additional advantage of increasing the robustness of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJFY9sKSAZu0"
   },
   "source": [
    "<!-- # Feed Forward Neural Networks -->\n",
    "\n",
    "<!-- ## Architecture -->\n",
    "\n",
    "<!-- ## Hyperparameters -->\n",
    "\n",
    "<!-- ## Backpropagation -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "so1Riv0wAZu0"
   },
   "source": [
    "<!-- # Convolutional Neural Networks -->\n",
    "\n",
    "<!-- ## Architecture -->\n",
    "\n",
    "<!-- ### Pooling layer -->\n",
    "\n",
    "<!-- ### Flatten layer -->\n",
    "\n",
    "<!-- ## Hyperparameters -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHSK1HTCAZu1"
   },
   "source": [
    "<!-- # Recurrent Neural Networks -->\n",
    "\n",
    "<!-- ## Architecture -->\n",
    "\n",
    "<!-- ## Backpropagation -->\n",
    "\n",
    "<!-- ### Exploding / Vanishing Gradients -->\n",
    "\n",
    "<!-- ## LSTMN -->\n",
    "\n",
    "<!-- ## NLP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHSK1HTCAZu1"
   },
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dl-intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
